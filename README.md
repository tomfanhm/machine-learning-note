## 資料預處理的主要步驟包括:

1. 資料清洗(Data Cleaning):處理缺失值、異常值、重複資料等
2. 資料轉換(Data Transformation):標準化、歸一化、編碼等
3. 資料集分割(Data Splitting):訓練集、驗證集、測試集的分割
4. 特徵選擇與降維(Feature Selection & Dimensionality Reduction)

## 資料預處理的目標是:

1. 提升資料質量,減少噪音
2. 提升模型的訓練效率,加速收斂
   收斂(Convergence)指的是一個算法或模型在訓練過程中逐漸趨近於一個穩定解或最優解的過程
3. 提升模型的泛化能力,避免過度擬合(Over fitting)

## 為什麼會有缺失值？

數據可能會因以下原因而缺失:

1. 人為錯誤(輸入資料時遺失)
2. 設備故障(感測器損壞或網路中斷)
3. 資料隱私(某些資訊未提供)

### 如何處理缺失值？

1. 刪除含缺失值的行
2. 填充平均數/中位數
3. 填充眾數
4. 前向/後向填充
5. 使用機器學習預測

## 什麼是異常值？

異常值是指與其他數據差距過大的值

### 如何檢測異常值？

1. 箱型圖(Boxplot)
2. 透過四分位數(IQR)偵測異常值
   IQR = Q3 – Q1(75% - 25%)
3. Z-score(標準分數)
4. 可視化

## 處理重複資料

為什麼會有重複數據？

1. 資料收集重複(同一個使用者提交了兩次資料)
2. 資料合併時重複(多個來源合併資料)
3. 資料庫儲存問題

### 如何處理重複資料？

1. 刪除完全重複的數據
2. 基於特定欄位去重,只保留最新交易記錄
3. 合併相似數據

## 資料轉換

### 資料縮放(Scaling)

如果不進行縮放,某些特徵的資料值遠大於其他特徵,會導致機器學習模型更關注較大數值的特徵,而忽略較小數值的特徵

### 兩種常見的縮放方法

1. 歸一化(Normalization)
2. 標準化(Standardization)

### 歸一化(Min-Max Scaling)

- 特點:將資料壓縮到 [0,1] 之間
- 適用場景:適用於神經網路(如深度學習)

### 標準化(Z-score Normalization)

- 特點:將資料轉換為平均值為 0,標準差為 1 的分佈
- 適用場景:適用於如線性迴歸、邏輯迴歸、KNN、SVM 等演算法

### 類別變數編碼(Categorical Encoding)

在機器學習中,演算法通常只能處理數值數據,所以我們需要對類別數據進行編碼

1. Label Encoding
   適用於有序類別:“低”=0, “中”=1, “高”=2
2. One-Hot Encoding
   適用於無序類別:[“北京”=1, “上海”=0, “深圳”=0]

## 資料集劃分(Data Splitting)

資料集的劃分方式

1. 訓練集(Training Set)
   用於訓練模型,讓模型學習資料中的模式
2. 驗證集(Validation Set)
   用於調參(Hyperparameter Tuning),選擇最佳模型
3. 測試集(Test Set)
   用於評估最終模型的泛化能力

### 資料集劃分的策略

1. 簡單分割

- 直接依比例劃分,例如 80% 訓練集,20% 測試集
- 優點:簡單、高效
- 缺點:測試集可能不夠代表整體資料分佈,影響模型評估

2. 交叉驗證(Cross-Validation)
   為了更穩定地評估模型表現,避免資料劃分所造成的偶然性誤差
   K 折交叉驗證(K-Fold Cross-Validation)

- 步驟:
  - 將資料分成 K 份(如 5 份)
  - 每次用 K-1 份作為訓練集,剩餘 1 份作為驗證集
  - 訓練 K 次,取平均結果
- 優點:更穩定,減少資料劃分的偶然性
- 缺點:計算成本較高

3. 留一法(Leave-One-Out Cross Validation, LOOCV)

- 每次只用 1 個資料點做測試集,其餘資料做訓練集
- 適用於小資料集(如醫學資料),但計算量較大

## K-近鄰(KNN)

它的核心思想是:

- 相似的樣本更可能屬於同一類別

### KNN 演算法的基本原理

1. 分類(Classification)
   KNN 透過投票機制來決定新數據點的類別:

- 計算新資料點與訓練資料集中所有點的距離
- 選擇最近的 K 個數據點
- 投票表決,哪個類別的數量最多,新資料點就屬於哪個類別

2. 回歸(Regression)
   KNN 也可以用於迴歸,即預測數值型資料:

- 計算新資料點與所有訓練資料的距離
- 選擇最近的 K 個數據點
- 計算 K 個鄰居的平均值,作為預測值

### KNN 中的關鍵參數

如何選擇適當的 K 值？

- K 太小(如 K=1):容易過度擬合(受雜訊影響大)
- K 太大(如 K=100):容易欠擬合(無法捕捉局部特徵)
- 常見經驗法則:
  經典做法:K 取資料集大小的開方 N
  交叉驗證:透過實驗選擇最優 K 值

### 如何計算數據點之間的距離？

1. 歐幾里德距離(Euclidean Distance)
2. 曼哈頓距離(Manhattan Distance)
3. 閔可夫斯基距離(Minkowski Distance)

### KNN 的優缺點

- 優點

  - 簡單易懂,不需要訓練過程,直接計算距離即可
  - 適用於小資料集,特別是結構清晰的數據
  - 可用於分類和迴歸

- 缺點
  - 計算量大,當資料量大時,每次查詢都要計算距離
  - 對維度高的資料不友好,在高維度空間距離計算會變得無效
  - 對異常值敏感,受雜訊資料影響大

## 線性迴歸(Linear Regression)

它假設自變數(特徵)與因變數(目標)之間是線性關係,即目標值可以用一個或多個特徵的加權和來表示

### 一元線性迴歸(Simple Linear Regression)

如果只有一個特徵(自變數),那麼線性迴歸的方程式是:
y = ax + b

- y:目標變數(預測值)
- x:輸入變數(特徵)
- a:斜率(迴歸係數,表示特徵對目標的影響)
- b:截距(當 x=0 時的 y 值)

### 多元線性迴歸(Multiple Linear Regression)

如果有多個特徵,迴歸方程式擴展為:
y = a1x + a2x + ... + b

### 線性迴歸的假設條件

線性迴歸有幾個重要的數學假設:

1. 線性關係(Linearity)

- 特徵與目標變數之間必須是線性關係(可用散佈圖檢查)

2. 獨立性(Independence)

- 觀察資料點是相互獨立的(時間序列資料通常違反這個假設)

3. 同方差性(Homoscedasticity)

- 誤差項的變異數應相等(可畫出殘差圖檢查)

4. 常態性(Normality)

- 誤差項應符合常態分佈

### 最小平方法(OLS)求解迴歸係數

線性迴歸的目標是找出最佳的 a 和 b ,使得預測值與真實值之間的誤差最小

1. 損失函數(均方誤差 MSE)
2. 最小平方法(OLS)公式求解

### 線性迴歸的優缺點

- 優點

  - 簡單易懂,計算量小,適合小資料集
  - 結果可解釋性強(能直接看出特徵的影響)
  - 適用於線性關係的數據

- 缺點
  - 假設過於嚴格,資料必須滿足線性關係
  - 對異常值敏感,離群點可能會影響迴歸係數
  - 無法處理複雜的非線性問題
